{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Dataset Information\n","metadata":{}},{"cell_type":"markdown","source":"****The objective of this problem is to predict the monetary value of a house located the boston suburbs.****","metadata":{}},{"cell_type":"markdown","source":"## Import modules","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\n%matplotlib inline\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2022-06-08T14:55:31.341984Z","iopub.execute_input":"2022-06-08T14:55:31.342453Z","iopub.status.idle":"2022-06-08T14:55:32.546872Z","shell.execute_reply.started":"2022-06-08T14:55:31.342346Z","shell.execute_reply":"2022-06-08T14:55:32.545702Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## Loading the dataset","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/boston-dataset/Boston.csv\")\ndf.drop(columns=['Unnamed: 0'], axis=0, inplace=True) #dropping unnamed\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-08T14:55:32.549784Z","iopub.execute_input":"2022-06-08T14:55:32.550718Z","iopub.status.idle":"2022-06-08T14:55:32.607848Z","shell.execute_reply.started":"2022-06-08T14:55:32.550675Z","shell.execute_reply":"2022-06-08T14:55:32.606645Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# statistical info\ndf.describe()","metadata":{"execution":{"iopub.status.busy":"2022-06-08T14:55:32.609273Z","iopub.execute_input":"2022-06-08T14:55:32.609846Z","iopub.status.idle":"2022-06-08T14:55:32.675112Z","shell.execute_reply.started":"2022-06-08T14:55:32.609801Z","shell.execute_reply":"2022-06-08T14:55:32.673812Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# datatype info\ndf.info()","metadata":{"execution":{"iopub.status.busy":"2022-06-08T14:55:32.677891Z","iopub.execute_input":"2022-06-08T14:55:32.678729Z","iopub.status.idle":"2022-06-08T14:55:32.699523Z","shell.execute_reply.started":"2022-06-08T14:55:32.678682Z","shell.execute_reply":"2022-06-08T14:55:32.698498Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"****All the columns are in numerical datatype.****\n****We will create new categorical columns using the existing columns later.****","metadata":{}},{"cell_type":"markdown","source":"## Preprocessing the dataset","metadata":{}},{"cell_type":"code","source":"# check for null values\ndf.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-06-08T14:55:32.701390Z","iopub.execute_input":"2022-06-08T14:55:32.702434Z","iopub.status.idle":"2022-06-08T14:55:32.711668Z","shell.execute_reply.started":"2022-06-08T14:55:32.702358Z","shell.execute_reply":"2022-06-08T14:55:32.710825Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"#no null values hence the dataset is clean and we can move forward to exploratory data analysis","metadata":{"execution":{"iopub.status.busy":"2022-06-08T14:55:32.712939Z","iopub.execute_input":"2022-06-08T14:55:32.713522Z","iopub.status.idle":"2022-06-08T14:55:32.724964Z","shell.execute_reply.started":"2022-06-08T14:55:32.713485Z","shell.execute_reply":"2022-06-08T14:55:32.724110Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"# create box plots\nfig, ax = plt.subplots(ncols=7, nrows=2, figsize=(20, 10))\nindex = 0\nax = ax.flatten()\n\nfor col, value in df.items():\n    sns.boxplot(y=col, data=df, ax=ax[index])\n    index += 1\nplt.tight_layout(pad=0.5, w_pad=0.7, h_pad=5.0)","metadata":{"execution":{"iopub.status.busy":"2022-06-08T14:55:32.726819Z","iopub.execute_input":"2022-06-08T14:55:32.727611Z","iopub.status.idle":"2022-06-08T14:55:34.272430Z","shell.execute_reply.started":"2022-06-08T14:55:32.727560Z","shell.execute_reply":"2022-06-08T14:55:34.271471Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"****In the graph, the dots represent the outliers.****\n\n****The column containing many outliers does not follow the normal distribution.****\n\n****We can minimalize outliers with log transformation.****\n\n****We can also drop the column which contains outliers (or) we can delete the rows which contains the same.****","metadata":{}},{"cell_type":"code","source":"# create dist plot\nfig, ax = plt.subplots(ncols=7, nrows=2, figsize=(20, 10))\nindex = 0\nax = ax.flatten()\n\nfor col, value in df.items():\n    sns.distplot(value, ax=ax[index])\n    index += 1\nplt.tight_layout(pad=0.5, w_pad=0.7, h_pad=5.0)","metadata":{"execution":{"iopub.status.busy":"2022-06-08T14:55:34.273714Z","iopub.execute_input":"2022-06-08T14:55:34.274159Z","iopub.status.idle":"2022-06-08T14:55:37.121029Z","shell.execute_reply.started":"2022-06-08T14:55:34.274125Z","shell.execute_reply":"2022-06-08T14:55:37.119663Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"****We can observe right skewed and left skewed graphs for 'crim', 'zn', 'tax', and 'black'.****\n\n****Therefore, we need to normalize these data.****\n\n","metadata":{}},{"cell_type":"markdown","source":"## Min-Max Normalization","metadata":{}},{"cell_type":"markdown","source":"****We will create the column list for the 4 columns and use Min-Max Normalization.****","metadata":{}},{"cell_type":"code","source":"cols = ['crim', 'zn', 'tax', 'black']\nfor col in cols:\n    # find minimum and maximum of that column\n    minimum = min(df[col])\n    maximum = max(df[col])\n    df[col] = (df[col] - minimum) / (maximum - minimum)","metadata":{"execution":{"iopub.status.busy":"2022-06-08T14:55:37.122451Z","iopub.execute_input":"2022-06-08T14:55:37.123425Z","iopub.status.idle":"2022-06-08T14:55:37.133793Z","shell.execute_reply.started":"2022-06-08T14:55:37.123316Z","shell.execute_reply":"2022-06-08T14:55:37.132665Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"****The last line shows the formula for min-max normalization.****\n\n****It will execute this code for the selected 4 columns.****","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(ncols=7, nrows=2, figsize=(20, 10))\nindex = 0\nax = ax.flatten()\n\nfor col, value in df.items():\n    sns.distplot(value, ax=ax[index])\n    index += 1\nplt.tight_layout(pad=0.5, w_pad=0.7, h_pad=5.0)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-06-08T14:55:37.136783Z","iopub.execute_input":"2022-06-08T14:55:37.137148Z","iopub.status.idle":"2022-06-08T14:55:40.134243Z","shell.execute_reply.started":"2022-06-08T14:55:37.137116Z","shell.execute_reply":"2022-06-08T14:55:40.133182Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# standardization\nfrom sklearn import preprocessing\nscalar = preprocessing.StandardScaler()\n\n# fit our data\nscaled_cols = scalar.fit_transform(df[cols])\nscaled_cols = pd.DataFrame(scaled_cols, columns=cols)\nscaled_cols.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-08T14:55:40.135387Z","iopub.execute_input":"2022-06-08T14:55:40.135744Z","iopub.status.idle":"2022-06-08T14:55:40.311195Z","shell.execute_reply.started":"2022-06-08T14:55:40.135711Z","shell.execute_reply":"2022-06-08T14:55:40.310094Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"for col in cols:\n    df[col] = scaled_cols[col] #assigning the values in orignal dataframe for further processing","metadata":{"execution":{"iopub.status.busy":"2022-06-08T14:55:40.312704Z","iopub.execute_input":"2022-06-08T14:55:40.313177Z","iopub.status.idle":"2022-06-08T14:55:40.321050Z","shell.execute_reply.started":"2022-06-08T14:55:40.313129Z","shell.execute_reply":"2022-06-08T14:55:40.319536Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(ncols=7, nrows=2, figsize=(20, 10))\nindex = 0\nax = ax.flatten()\n\nfor col, value in df.items():\n    sns.distplot(value, ax=ax[index])\n    index += 1\nplt.tight_layout(pad=0.5, w_pad=0.7, h_pad=5.0)","metadata":{"execution":{"iopub.status.busy":"2022-06-08T14:55:40.322609Z","iopub.execute_input":"2022-06-08T14:55:40.323545Z","iopub.status.idle":"2022-06-08T14:55:43.221623Z","shell.execute_reply.started":"2022-06-08T14:55:40.323498Z","shell.execute_reply":"2022-06-08T14:55:43.220601Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"#Standardization uses mean and standard deviation. Here, preprocessing.StandardScaler( ) \n#is the standardization function.\n#Even now the columns 'crim', 'zn', 'tax', and 'black' does not show a perfect normal distribution.\n\n#However, the standardized value of these columns will slightly improve the model performance.","metadata":{"execution":{"iopub.status.busy":"2022-06-08T14:55:43.223529Z","iopub.execute_input":"2022-06-08T14:55:43.224237Z","iopub.status.idle":"2022-06-08T14:55:43.229092Z","shell.execute_reply.started":"2022-06-08T14:55:43.224188Z","shell.execute_reply":"2022-06-08T14:55:43.228027Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## Coorelation Matrix\n\n","metadata":{}},{"cell_type":"code","source":"corr = df.corr()\nplt.figure(figsize=(20,10))\nsns.heatmap(corr, annot=True, cmap='coolwarm')","metadata":{"execution":{"iopub.status.busy":"2022-06-08T14:55:43.230903Z","iopub.execute_input":"2022-06-08T14:55:43.231540Z","iopub.status.idle":"2022-06-08T14:55:44.474038Z","shell.execute_reply.started":"2022-06-08T14:55:43.231494Z","shell.execute_reply":"2022-06-08T14:55:44.473262Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"****We mostly focus on the target variable as this is a Regression problem.****\n\n****But we can also observe other highly correlated attributes by column 'tax' and 'rad'.****\n\n****We will later eliminate this correlation by ignoring any of the variables.****\n\n****Additionally, we will display 'lstat' and 'rm' to show their correlation with the target variable 'medv'.****\n\n","metadata":{}},{"cell_type":"code","source":"sns.regplot(y=df['medv'], x=df['lstat'])","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-06-08T14:55:44.475210Z","iopub.execute_input":"2022-06-08T14:55:44.475654Z","iopub.status.idle":"2022-06-08T14:55:44.822312Z","shell.execute_reply.started":"2022-06-08T14:55:44.475624Z","shell.execute_reply":"2022-06-08T14:55:44.821329Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"#Here, the price of houses decreases with the increase in the 'lstat'.\n#Hence it is negatively correlated. lstat s the poulation hence its obvious\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.regplot(y=df['medv'], x=df['rm'])","metadata":{"execution":{"iopub.status.busy":"2022-06-08T14:55:44.823452Z","iopub.execute_input":"2022-06-08T14:55:44.823788Z","iopub.status.idle":"2022-06-08T14:55:45.150154Z","shell.execute_reply.started":"2022-06-08T14:55:44.823758Z","shell.execute_reply":"2022-06-08T14:55:45.149236Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"#Here, the prices of houses increase with the increase in 'rm'.\n#Hence it is positively correlated. that is rm is rooms per dwelling","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Input Split","metadata":{}},{"cell_type":"code","source":"X = df.drop(columns=['medv', 'rad'], axis=1)\ny = df['medv']","metadata":{"execution":{"iopub.status.busy":"2022-06-08T15:02:33.207214Z","iopub.execute_input":"2022-06-08T15:02:33.208340Z","iopub.status.idle":"2022-06-08T15:02:33.214815Z","shell.execute_reply.started":"2022-06-08T15:02:33.208287Z","shell.execute_reply":"2022-06-08T15:02:33.213993Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"## Model Training","metadata":{}},{"cell_type":"markdown","source":"Instead of training the whole model, we will split the dataset for estimating the model performance.\n\nIf you train and test the dataset completely, the results will be inaccurate. Hence, we will use 'train_test_split'.\n\nWe will add random_state with the attribute 42 to get the same split upon re-running.\n\nIf you don't specify a random state, it will randomly split the data upon re-running giving inconsistent resu","metadata":{"execution":{"iopub.status.busy":"2022-06-08T15:03:33.818196Z","iopub.execute_input":"2022-06-08T15:03:33.818580Z","iopub.status.idle":"2022-06-08T15:03:33.828255Z","shell.execute_reply.started":"2022-06-08T15:03:33.818550Z","shell.execute_reply":"2022-06-08T15:03:33.826179Z"}}},{"cell_type":"markdown","source":"[](http://)","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\ndef train(model, X, y):\n    # train the model\n    x_train, x_test, y_train, y_test = train_test_split(X, y, random_state=42)\n    model.fit(x_train, y_train)\n    \n    # predict the training set\n    pred = model.predict(x_test)\n    \n    # perform cross-validation\n    cv_score = cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=5)\n    cv_score = np.abs(np.mean(cv_score))\n    \n    print(\"Model Report\")\n    print(\"MSE:\",mean_squared_error(y_test, pred))\n    print('CV Score:', cv_score)","metadata":{"execution":{"iopub.status.busy":"2022-06-08T15:15:45.975558Z","iopub.execute_input":"2022-06-08T15:15:45.976853Z","iopub.status.idle":"2022-06-08T15:15:45.985666Z","shell.execute_reply.started":"2022-06-08T15:15:45.976794Z","shell.execute_reply":"2022-06-08T15:15:45.984131Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"X contains input attributes and y contains the output attribute.\n\nWe use 'cross val score' for better validation of the model.\n\nHere, cv=5 means that the cross-validation will split the data into 5 parts.\n\nnp.abs will convert the negative score to positive and np.mean will give the average value of 5 scores.\n\n","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nmodel = LinearRegression(normalize=True)\ntrain(model, X, y)\ncoef = pd.Series(model.coef_, X.columns).sort_values()\ncoef.plot(kind='bar', title='Model Coefficients')\n","metadata":{"execution":{"iopub.status.busy":"2022-06-08T15:17:06.899940Z","iopub.execute_input":"2022-06-08T15:17:06.900442Z","iopub.status.idle":"2022-06-08T15:17:07.159333Z","shell.execute_reply.started":"2022-06-08T15:17:06.900377Z","shell.execute_reply":"2022-06-08T15:17:07.158528Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\nmodel = DecisionTreeRegressor()\ntrain(model, X, y)\ncoef = pd.Series(model.feature_importances_, X.columns).sort_values(ascending=False)\ncoef.plot(kind='bar', title='Feature Importance')","metadata":{"execution":{"iopub.status.busy":"2022-06-08T14:55:45.615893Z","iopub.execute_input":"2022-06-08T14:55:45.616396Z","iopub.status.idle":"2022-06-08T14:55:45.964275Z","shell.execute_reply.started":"2022-06-08T14:55:45.616361Z","shell.execute_reply":"2022-06-08T14:55:45.963245Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nmodel = RandomForestRegressor()\ntrain(model, X, y)\ncoef = pd.Series(model.feature_importances_, X.columns).sort_values(ascending=False)\ncoef.plot(kind='bar', title='Feature Importance')","metadata":{"execution":{"iopub.status.busy":"2022-06-08T14:55:45.965537Z","iopub.execute_input":"2022-06-08T14:55:45.965881Z","iopub.status.idle":"2022-06-08T14:55:48.242178Z","shell.execute_reply.started":"2022-06-08T14:55:45.965851Z","shell.execute_reply":"2022-06-08T14:55:48.241468Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesRegressor\nmodel = ExtraTreesRegressor()\ntrain(model, X, y)\ncoef = pd.Series(model.feature_importances_, X.columns).sort_values(ascending=False)\ncoef.plot(kind='bar', title='Feature Importance')","metadata":{"execution":{"iopub.status.busy":"2022-06-08T14:55:48.243387Z","iopub.execute_input":"2022-06-08T14:55:48.244223Z","iopub.status.idle":"2022-06-08T14:55:49.719956Z","shell.execute_reply.started":"2022-06-08T14:55:48.244184Z","shell.execute_reply":"2022-06-08T14:55:49.719158Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"import xgboost as xgb\nmodel = xgb.XGBRegressor()\ntrain(model, X, y)\ncoef = pd.Series(model.feature_importances_, X.columns).sort_values(ascending=False)\ncoef.plot(kind='bar', title='Feature Importance')","metadata":{"execution":{"iopub.status.busy":"2022-06-08T14:55:49.721050Z","iopub.execute_input":"2022-06-08T14:55:49.721789Z","iopub.status.idle":"2022-06-08T14:55:52.045635Z","shell.execute_reply.started":"2022-06-08T14:55:49.721745Z","shell.execute_reply":"2022-06-08T14:55:52.044489Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"****Final Thoughts\nTo summarize,RandomForestRegressor  works best for this project.****\n\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}